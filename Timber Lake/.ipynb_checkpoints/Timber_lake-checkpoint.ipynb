{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "be1a6705-a41f-4379-a831-1fae4a2979a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a14f2632-a177-42df-a437-95108044c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Website_name='Timber Lake Camp'\n",
    "Website_url='https://www.timberlakecamp.com/'\n",
    "HEADERS =({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "                   'Accept-Language':'en-US, en;q=0.5'})\n",
    "\n",
    "home = requests.get(Website_url,headers=HEADERS)\n",
    "home_soup = BeautifulSoup(home.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a25e9dee-c383-4d58-8551-8628eed6c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store descriptions\n",
    "description = []\n",
    "\n",
    "# Iterate through the description_table\n",
    "for i in description_table:\n",
    "    desc = i.find_all('p')  # Find all 'p' tags\n",
    "    # Append the text from each <p> tag to the list if it's not empty\n",
    "    for d in desc:\n",
    "        text = d.text.strip()  # Stripping extra whitespace\n",
    "        if text:  # Check if the text is not empty\n",
    "            description.append(text)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b1c6faf9-db04-46de-997f-e2f5605c6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summer\n",
      "\n",
      "354Â Timber Lake Road\n",
      "Shandaken, NY 12480\n",
      "845-688-2266\n",
      "Winter\n",
      "\n",
      "85 Crescent Beach Road\n",
      "Glen Cove, NY 11542\n",
      "516-656-4200\n"
     ]
    }
   ],
   "source": [
    "location=[]\n",
    "location1=home_soup.find('section',attrs={'class':'col-sm-3 col-sm-pull-6'})\n",
    "print(location1.text.strip())\n",
    "location2=home_soup.find('section',attrs={'class':'right col-sm-3'})\n",
    "print(location2.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e6ce4950-e7b7-4444-8972-43ac81114248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location: ['354 Timber Lake Road Shandaken, NY 12480', '85 Crescent Beach Road Glen Cove, NY 11542']\n",
      "telephone: ['845-688-2266', '516-656-4200']\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store addresses and phone numbers\n",
    "addresses = []\n",
    "phone_numbers = []\n",
    "\n",
    "# Extract location1 and location2 details\n",
    "location1 = home_soup.find('section', attrs={'class': 'col-sm-3 col-sm-pull-6'}).text.strip()\n",
    "location2 = home_soup.find('section', attrs={'class': 'right col-sm-3'}).text.strip()\n",
    "\n",
    "# Combine both locations for processing\n",
    "all_locations = location1 + \"\\n\" + location2\n",
    "\n",
    "# Process the text to extract addresses and phone numbers\n",
    "lines = all_locations.split(\"\\n\")\n",
    "current_address = \"\"\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip().replace('\\xa0', ' ')  # Clean and normalize the line\n",
    "    if line.isdigit() or (line.replace(\"-\", \"\").isdigit() and len(line) >= 10):  # Check for phone numbers\n",
    "        phone_numbers.append(line)\n",
    "        if current_address:  # Append the current address when a phone number is found\n",
    "            addresses.append(current_address.strip())\n",
    "            current_address = \"\"\n",
    "    elif line:  # If the line contains text (address or name)\n",
    "        if not any(keyword in line.lower() for keyword in [\"summer\", \"winter\"]):  # Skip location names\n",
    "            current_address += \" \" + line\n",
    "\n",
    "# Ensure the last address is added if any\n",
    "if current_address:\n",
    "    addresses.append(current_address.strip())\n",
    "\n",
    "# Print the results\n",
    "print(\"location:\", addresses)\n",
    "print(\"telephone:\", phone_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d45e83c-65af-45ab-b134-fdb8b5ed2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail='info@timberlakecamp.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "39adb664-6b24-47f7-9a22-116461a07947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facebook:http://www.facebook.com/timberlakecamp',\n",
       " 'twitter:http://www.twitter.com/timberlakecamp',\n",
       " 'instagram:https://www.instagram.com/timberlakecamp/',\n",
       " 'youtube:https://www.youtube.com/user/TimberLakeCampVideos']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store social media links\n",
    "socials = []\n",
    "\n",
    "# Find the section containing the social media links\n",
    "social_section = home_soup.find('section', attrs={'class': 'col-sm-4 col-md-3 right socials'})\n",
    "\n",
    "# Find all the <a> tags within the section\n",
    "social_links = social_section.find_all('a', href=True)\n",
    "\n",
    "# Iterate through the links and store them in the desired format\n",
    "for link in social_links:\n",
    "    href = link['href']\n",
    "    if \"facebook\" in href:\n",
    "        socials.append(f\"facebook:{href}\")\n",
    "    elif \"twitter\" in href:\n",
    "        socials.append(f\"twitter:{href}\")\n",
    "    elif \"instagram\" in href:\n",
    "        socials.append(f\"instagram:{href}\")\n",
    "    elif \"youtube\" in href:\n",
    "        socials.append(f\"youtube:{href}\")\n",
    "\n",
    "socials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7d31667a-933f-4b75-8d99-df61f8642819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_date': 'Saturday, June 28th, 2025',\n",
       " 'end_date': 'Saturday, August 16th, 2025',\n",
       " 'fee': '$16,000',\n",
       " 'deposit': '$5,500'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL and headers\n",
    "rd_url = 'https://www.timberlakecamp.com/prospective-families/dates-fees-faqs/'\n",
    "# Fetch and parse the page content\n",
    "rd = requests.get(rd_url, headers=HEADERS)\n",
    "rd_soup = BeautifulSoup(rd.content, 'lxml')\n",
    "\n",
    "# Extract dates\n",
    "dates_table = rd_soup.find('table', id='tablepress-19')\n",
    "rows = dates_table.find_all('tr')[1:]  # Skip header row\n",
    "start_date = rows[0].find_all('td')[1].text.strip()\n",
    "end_date = rows[2].find_all('td')[1].text.strip()\n",
    "\n",
    "# Extract fees and deposit\n",
    "fees_table = rd_soup.find('table', id='tablepress-4')\n",
    "fee = fees_table.find_all('td')[0].find('strong').text.strip()\n",
    "\n",
    "deposit_table = fees_table.find_all('td')[1]\n",
    "deposit_text = deposit_table.text.strip()\n",
    "deposit = deposit_text.split('deposit on enrollment')[0].strip()  # Extract deposit amount\n",
    "\n",
    "# Create a dictionary with extracted details\n",
    "camp_details = {\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'fee': fee,\n",
    "    'deposit': deposit,\n",
    "}\n",
    "\n",
    "camp_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d601c-85d4-4a9e-9f5e-651ddbc266f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = 'https://www.timberlakecamp.com/prospective-families/program-activities/'\n",
    "\n",
    "# Send GET request to fetch the main page\n",
    "cat = requests.get(cat_url, headers=HEADERS)\n",
    "cat_soup = BeautifulSoup(cat.content, 'lxml')\n",
    "\n",
    "categories = []\n",
    "\n",
    "# Locate all the relevant div elements\n",
    "divs = cat_soup.find_all(\"div\", class_=\"ezcol\")\n",
    "\n",
    "for div in divs:\n",
    "    # Extract category name (from h3 or nested a tag)\n",
    "    category_tag = div.find('h3')\n",
    "    category = category_tag.get_text(strip=True) if category_tag else None\n",
    "\n",
    "    # Extract image URL (handle lazy-loaded images)\n",
    "    img_tag = div.find('img')\n",
    "    image_url = img_tag.get('data-lazy-src') or img_tag.get('data-src') or img_tag.get('src') if img_tag else None\n",
    "\n",
    "    # Extract the URL of the individual description page\n",
    "    desc_url = div.find('a').get('href') if div.find('a') else None\n",
    "\n",
    "    # Proceed if there is a valid URL for the description page\n",
    "    if category and image_url and desc_url:\n",
    "        # Check if the category is already in the list to avoid duplicates\n",
    "        existing_category = next((item for item in categories if item['name'] == category), None)\n",
    "        \n",
    "        if existing_category:\n",
    "            continue  # Skip if this category is already in the list\n",
    "        \n",
    "        # Scrape the description page if it's not already in the list\n",
    "        desc = requests.get(desc_url, headers=HEADERS)\n",
    "        desc_soup = BeautifulSoup(desc.content, 'lxml')\n",
    "\n",
    "        # Extract the description from the description page\n",
    "        heading = desc_soup.find('h1').get_text(strip=True) if desc_soup.find('h1') else 'No heading found'\n",
    "        \n",
    "        # Extract paragraph content (likely the description)\n",
    "        paragraphs = desc_soup.find_all('p')\n",
    "        description = \"\"\n",
    "        for para in paragraphs:\n",
    "            description += para.get_text(strip=True) + \"\\n\"\n",
    "        \n",
    "        # Clean the description by removing unwanted phrases and whitespace\n",
    "        description = description.replace(\"Â© Copyright Timber Lake Camp. All rights reserved.\", \"\")\n",
    "        description = description.replace(\"Website by\", \"\")\n",
    "        description = description.replace(\"We canât wait to hear from you!\", \"\")\n",
    "        description = description.replace(\"Fields marked with*are required.\", \"\")\n",
    "        description = ' '.join(description.split())  # Remove excess whitespace\n",
    "\n",
    "        # Add cleaned description to the category data\n",
    "        category_data = {'name': category, 'description': description, 'image': image_url}\n",
    "        categories.append(category_data)\n",
    "\n",
    "categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6dea30c9-2038-44ad-876a-4fb77b6d29ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_url = 'https://www.timberlakecamp.com/prospective-families/program-activities/electives/'\n",
    "\n",
    "# Send GET request to fetch the page content\n",
    "a = requests.get(act_url, headers=HEADERS)\n",
    "act_soup = BeautifulSoup(a.content, 'lxml')\n",
    "\n",
    "# Find all relevant divs containing activities\n",
    "act_table = act_soup.find_all('div', attrs={'class': 'ezcol'})\n",
    "\n",
    "# Initialize the activities list\n",
    "all_activities = []\n",
    "\n",
    "# Loop through each set of activities\n",
    "for activities_set in Activities:\n",
    "    # Split by the newline character and clean the entries\n",
    "    activities = activities_set.split('\\n')\n",
    "    \n",
    "    # Add each activity individually to the all_activities list\n",
    "    for activity in activities:\n",
    "        all_activities.append(activity.strip())  # Remove any extra spaces\n",
    "\n",
    "# Display the list of all individual activities\n",
    "all_activities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
